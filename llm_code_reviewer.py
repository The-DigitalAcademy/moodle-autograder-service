import google.generativeai as genai
import os, json, ast
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Load Gemini API configuration
API_KEY = os.getenv('GEMINI_API_KEY')
MODEL = os.getenv('GEMINI_MODEL', 'gemini-2.5-flash')

# Configure Google Generative AI
genai.configure(api_key=API_KEY)


class LLMCodeReviewer:
    """
    Automatically evaluates and reviews student code submissions using
    a Large Language Model (Gemini). The class builds a structured prompt
    that includes a rubric, activity instructions, and student code files.
    """

    def __init__(self, files: list[dict], rubric: str, activity_instruction: str, output_template: str):

        """
        Initialize the LLMCodeReviewer.

        Args:
            files (list[dict]): List of code files, each with structure:
                {
                    "name": str,    # file name
                    "path": str,    # relative path in repo
                    "content": str  # code content
                }
            rubric (str): Grading rubric text describing evaluation criteria.
            activity_instruction (str): Instructions for the coding activity.
            output_template (str): Expected format or JSON schema for output.
        """
        
        self.files = files
        self.rubric = rubric
        self.activity_instruction = activity_instruction
        self.output_template = output_template
        self.model = genai.GenerativeModel(MODEL)
        
        
        # === Construct Prompt Sections ===
        prompt_intro = "You are an expert programming instructor and automatic grader."
        prompt_task_definition = """
### Task definition
- Grade the following student's code using the given rubric guide and the weightings set for each criterion.
- Apply each criterion fairly.
- You may simulate test cases mentally; do not run code.
- Be concise and constructive.
"""
        prompt_rubric_guide = f"### Rubric\n{self.rubric}"
        prompt_activity_instruction = f"###Activity Instructions for the learner\n{self.activity_instruction}"
        prompt_repsponse_guide = f"### Response Template\nRespond **only** using the following format:\n{self.output_template}"
        prompt_code_content = f"### Student Code:\n{self.get_file_contents()}"

        # === Combine all parts into the final prompt ===
        self.prompt = (
            f"{prompt_intro}\n"
            f"{prompt_task_definition}\n"
            f"{prompt_rubric_guide}\n"
            f"{prompt_activity_instruction}\n"
            f"{prompt_repsponse_guide}\n"
            f"{prompt_code_content}\n"
        )

    def get_file_contents(self):
        """
        Combine all file contents into a readable Markdown-style text block
        for inclusion in the prompt.

        Returns:
            str: Concatenated text of all files in the format:
                <path>
                ```
                <content>
                ```
        """
        file_contents_in_text = ''
        for file in self.files:
            file_contents_in_text += f"\n\n{file["path"]}\n```\n{file["content"]}\n```\n"

        return file_contents_in_text
    
    def get_structured_review(self):
        """
        Generate a code review and grade response from the Gemini model.

        Returns:
            str: The text output generated by the model.
        """
        try:
            response = self.model.generate_content(self.prompt)
            response_text = response.text
            if response_text.startswith("```json"):
                response_text = response_text[7:].rsplit("```", 1)[0].strip()
            return self._safe_parse_text_to_json(response_text)
        except Exception as e:
            print(f"Gemini model call failed. {e}")
            return f"Error: {str(e)}"


    def get_prompt(self):
        return self.prompt

    def _safe_parse_text_to_json(self, text: str) -> dict:
        """
        Safely parse model output text to JSON or Python dict.

        Attempts JSON parsing first, then falls back to `ast.literal_eval`.
        If both fail, returns a dict containing the raw text snippet.

        Args:
            text (str): Model response text.

        Returns:
            dict: Parsed JSON/dict data or error information.
        """
        try:
            return json.loads(text)
        except Exception:
            try:
                return ast.literal_eval(text)
            except Exception:
                return {"error": "failed_to_parse", "raw": text[:1000]}